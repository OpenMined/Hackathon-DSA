{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56c60e9-fe84-4d03-bf05-0bd36c08e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain chromadb ctransformers transformers sentence_transformers\n",
    "# Apple silicon:\n",
    "# !pip uninstall ctransformers\n",
    "# !CT_METAL=1 pip install ctransformers --no-binary ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d3155f-1e4b-4f01-9056-05e2be6aaa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "SNAPSHOTS_DIR = DATA_DIR / \"platform-docs-snapshots\"\n",
    "VERSIONS_DIR = DATA_DIR / \"platform-docs-versions\"\n",
    "\n",
    "# hparams\n",
    "chunk_size = 1024\n",
    "chunk_overlap = chunk_size // 10\n",
    "embedder_name = \"BAAI/bge-small-en-v1.5\" # https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece42cd-9d32-4eda-99a6-984393c105aa",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "febb8e8b-c1aa-4bb1-9230-2a5ddf97dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader, DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "docs = DirectoryLoader(VERSIONS_DIR, glob=\"[!.]*/[!.]*.md\", loader_cls=TextLoader).load()\n",
    "docs = [d for d in docs if Path(d.metadata['source']) != VERSIONS_DIR / \"README.md\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f657de3-657f-47e0-988f-e4b81b6ad532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n",
      "11598\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "docs_split = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(docs))\n",
    "print(len(docs_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f7f3b-0bc4-41f2-bf24-e316d73e2a44",
   "metadata": {},
   "source": [
    "# Build index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19c65cf-530a-4dc3-aa80-2842fa0aebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import re\n",
    "\n",
    "# Chroma uses all-MiniLM-L6-v2 by default\n",
    "chroma_client = chromadb.PersistentClient()\n",
    " \n",
    "def format_model_name(name):\n",
    "    # chromaDB only allows these characters\n",
    "    return re.sub(r'[^a-zA-Z0-9_-]', '_', name)\n",
    "\n",
    "collection_name = f\"DSA_{format_model_name(embedder_name)}\"\n",
    "\n",
    "try:\n",
    "    collection = chroma_client.get_collection(collection_name)\n",
    "except ValueError:\n",
    "    print(\"Building collection...\")\n",
    "    embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(embedder_name=embedder_name)\n",
    "\n",
    "    collection = chroma_client.create_collection(name=collection_name, embedding_function=embedding_fn)\n",
    "    \n",
    "    text = [d.page_content for d in docs_split]\n",
    "    metadatas = [d.metadata for d in docs_split]\n",
    "    ids = [uuid.uuid4().hex for _ in range(len(docs_split))]\n",
    "    \n",
    "    collection.add(\n",
    "        documents=text,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "815650d9-38b6-4792-9c77-4f0230755f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"How do I get activity logs from the Twitter API?\"],\n",
    "    n_results=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e873b9d-5843-4a94-a6fa-ef436eee2aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'source': '../data/platform-docs-versions/X_Twitter-API-V1/Tweets.md'},\n",
      "  {'source': '../data/platform-docs-versions/X_Twitter-API-V2/Tweets.md'},\n",
      "  {'source': '../data/platform-docs-versions/X_Twitter-API-V1/Tweets.md'},\n",
      "  {'source': '../data/platform-docs-versions/X_Twitter-API-V2/Users.md'},\n",
      "  {'source': '../data/platform-docs-versions/X_Twitter-API-V2/Tweets.md'}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Working with timelines  \\n\\n-------------------------\\n\\nThe Twitter API has several methods, such as\\xa0[GET statuses / user\\\\_timeline](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline.html)\\xa0and\\xa0[GET statuses / home\\\\_timeline](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline.html), which return a timeline of Tweet data. Such timelines can grow very large, so there are limits to how much of a timeline a client application may fetch in a single request. Applications must therefore iterate through timeline results in order to build a more complete list.\\n\\nBecause of Twitter’s realtime nature and the volume of data which is constantly being added to timelines, standard paging approaches are not always effective. The goal of this page is to demonstrate the issues Twitter developers may face when paging through result sets and to give best practices for processing a timeline.\\n\\n### The problem with “pages”'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint(results[\"metadatas\"])\n",
    "results[\"documents\"][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dfa1f7-f746-475c-9a88-6d86a6c31b35",
   "metadata": {},
   "source": [
    "# Load Generator\n",
    "\n",
    "### Quantized Mistral 7B, finetuned on code instructions\n",
    "- https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-16k-GGUF#provided-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9beb64f0-baab-4759-b1f1-19205dbbad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def print_chat(chat_log):\n",
    "    for entry in chat_log:\n",
    "        if entry['role'] != 'system':\n",
    "            display(Markdown(f\"**{entry['role'].capitalize()}:** \\n{entry['content']}\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "822ad186-df9b-499c-bd46-8575020afa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca8ca3e715841479bdada28d2960782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f79432ef80495fa9f1a814ae210fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "TEMPLATE = '''\n",
    "You are a QA assistant that answers questions based only on the context you are given.\n",
    "\n",
    "Examples:\n",
    "Q: I am a coding expert. How could I get more info about a Facebook post whose url slug ends with 123456789_123456789? I have a crowdtangle API token (TOKEN). Your output is a bash code snippet.\n",
    "A: ```bash\n",
    "curl -L https://api.crowdtangle.com/post/123456789_123456789?token=<CROWDTANGLE_API_TOKEN>\n",
    "```\n",
    "\n",
    "Next section will contain the context. You can use it to formulate the answer to the user question.\n",
    "------------------\n",
    "{context}\n",
    "------------------\n",
    "\n",
    "RULES\n",
    "- Give an answer ONLY based on the above context and with no prior knowledge.\n",
    "- If you cannot come up with an answer to the user question, answer \"I do not know the answer to this question\".\n",
    "- Your answers are short, complete and easy to follow. Include code examples if neccessary.\n",
    "'''\n",
    "\n",
    "class MistralRAGGenerator:\n",
    "    def __init__(self, gpu_layers: int = 0):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"TheBloke/OpenHermes-2.5-Mistral-7B-16k-GGUF\",\n",
    "            model_file=\"openhermes-2.5-mistral-7b-16k.Q4_K_M.gguf\",\n",
    "            model_type=\"mistral\", \n",
    "            gpu_layers=gpu_layers,\n",
    "            max_new_tokens=4000,\n",
    "            context_length=16_000,\n",
    "        )\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"NurtureAI/OpenHermes-2.5-Mistral-7B-16k\")\n",
    "\n",
    "    def format_prompt(self, messages: List[Dict[str, str]]) -> str:\n",
    "        return self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    def format_context(self, context: List[str]) -> str:\n",
    "        context_formatted = []\n",
    "        for i, c in enumerate(context):\n",
    "            context_formatted.append(f\"CONTEXT {i}\\n{c}\")\n",
    "\n",
    "        return \"\\n\\n\".join(context_formatted)\n",
    "\n",
    "    def init_chat_log(self, context: List[str], template: Optional[str] = None) -> List[Dict[str, str]]:\n",
    "        template = template or TEMPLATE\n",
    "        context = self.format_context(context)\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": template.format(context=context)},\n",
    "        ]\n",
    "    \n",
    "    def chat(self, chat_log: List[Dict[str, str]], query: str):\n",
    "        if query:\n",
    "            chat_log.append({\"role\": \"user\", \"content\": query})\n",
    "        if chat_log[-1][\"role\"] != \"user\":\n",
    "            raise ValueError(\"query required\")\n",
    "                \n",
    "        prompt = self.format_prompt(chat_log)\n",
    "        output = self.model(prompt, stop=[\"<|im_end|>\"])\n",
    "        chat_log.append({\"role\": \"assistant\", \"content\": output})\n",
    "        return chat_log\n",
    "    \n",
    "    def generate_answer(self, context: str, query: str) -> List[Dict[str, str]]:\n",
    "        chat_log = self.init_chat_log(context, template=\"You: {context}\\nMistral: \")\n",
    "        return self.chat(chat_log, query=query)\n",
    "\n",
    "\n",
    "rag_generator = MistralRAGGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3398395-f6c3-4c86-8d3d-1d63f011ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "query = \"Tell me how to work with WhatsApp business accounts using the Facebook Graph API\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=k,\n",
    ")\n",
    "\n",
    "context = results['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a82a9d9-7fec-46f6-8f3f-c9f1fa182841",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_log = rag_generator.generate_answer(context, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2d66110-21df-43a2-9af9-293cafc2b948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**User:** \n",
       "Tell me how to work with WhatsApp business accounts using the Facebook Graph API\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Assistant:** \n",
       "To work with WhatsApp Business Accounts using the Facebook Graph API, you need to have the following permissions: `whatsapp_business_management`, `whatsapp_messaging`, and `public_profile`. Additionally, you will need a valid user access token and the ID of the WhatsApp Business Account (WABA) you want to retrieve information for.\n",
       "\n",
       "To make a request using cURL, use the following command:\n",
       "```cURLAndroid SDKObjective-C\n",
       "    curl -i -X GET \\\n",
       "     \"https://graph.facebook.com/LATEST-VERSION/WHATSAPP-BUSINESS-ACCOUNT-ID?access_token=USER-ACCESS-TOKEN\"\n",
       "```\n",
       "Or using the SDK in Objective-C:\n",
       "```swift\n",
       "    GraphRequest request = GraphRequest.newGraphPathRequest(\n",
       "      accessToken,\n",
       "      \"/WHATSAPP-BUSINESS-ACCOUNT-ID\",\n",
       "      new GraphRequest.Callback() {\n",
       "        @Override\n",
       "        public void onCompleted(GraphResponse response) {\n",
       "          // Insert your code here\n",
       "        }\n",
       "    });\n",
       "    \n",
       "    request.executeAsync();\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_chat(chat_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a916457b-c3d5-4e14-9855-0408049b65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# - batch inputs, context, outputs in whole pipeline\n",
    "# - read input + write output + eval\n",
    "\n",
    "from data import get_training_data\n",
    "train_queries, train_answers, train_context = get_training_data()\n",
    "\n",
    "# model outputs\n",
    "chat_logs_batch = [chat_log, chat_log] # TODO dummy batch\n",
    "queries = [c[1][\"content\"] for c in chat_logs_batch]\n",
    "answers = [c[-1][\"content\"] for c in chat_logs_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72405741-3b35-47d6-a63a-5ff240922a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
