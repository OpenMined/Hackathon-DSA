{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512c46ad-bd13-4919-9ff1-9865b3ed971d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.37.tar.gz (10.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /home/eelco/.pyenv/versions/3.10.13/envs/dsa/lib/python3.10/site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/eelco/.pyenv/versions/3.10.13/envs/dsa/lib/python3.10/site-packages (from llama-cpp-python) (4.9.0)\n",
      "Collecting diskcache>=5.6.1\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /home/eelco/.pyenv/versions/3.10.13/envs/dsa/lib/python3.10/site-packages (from llama-cpp-python) (1.26.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/eelco/.pyenv/versions/3.10.13/envs/dsa/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.4)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.37-cp310-cp310-manylinux_2_35_x86_64.whl size=2508657 sha256=83183ceff3f7ab8fe609642a69ee90ef5bea26a90ae6c2c9f6c69c41138f2bbb\n",
      "  Stored in directory: /home/eelco/.cache/pip/wheels/5b/4f/36/1df917d3d21e5d04ce960616a6fb9e39c1548149620528a67c\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.37\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1385bae0-be3d-4e2b-9560-96173c600181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from ../models/towerinstruct-7b-v0.1.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32007]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32007]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32007]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32005\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 32001\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32004\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 266/32007 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32007\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32005 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: SEP token        = 32001 '<SEP>'\n",
      "llm_load_print_meta: PAD token        = 32004 '<PAD>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.28 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    16.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   308.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '<|im_end|>' + '\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '32004', 'tokenizer.ggml.seperator_token_id': '32001', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32005', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"../models/towerinstruct-7b-v0.1.Q4_K_M.gguf\",\n",
    "    n_ctx=4096,\n",
    "    n_threads=8,\n",
    "    n_gpu_layers=0,\n",
    "    chat_format=\"chatml\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f82d25b4-d626-4558-a3d9-aa1b5cb76ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3832.14 ms\n",
      "llama_print_timings:      sample time =       4.54 ms /    16 runs   (    0.28 ms per token,  3528.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     752.42 ms /    22 tokens (   34.20 ms per token,    29.24 tokens per second)\n",
      "llama_print_timings:        eval time =    2316.44 ms /    15 runs   (  154.43 ms per token,     6.48 tokens per second)\n",
      "llama_print_timings:       total time =    3105.19 ms /    37 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-2872fb26-78c3-491d-80f6-b3ee03044c8d',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1706696501,\n",
       " 'model': '../models/towerinstruct-7b-v0.1.Q4_K_M.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Combien de personnes des Pays-Bas sont actives sur X ?'},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 120, 'completion_tokens': 15, 'total_tokens': 135}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUESTION_SYSTEM_PROMPT=\"\"\"\n",
    "You are an assistant that translates between the requested languages.\n",
    "- If your translations contain code, only comments are translated.\n",
    "- API calls and code should be left untouched to remain functional\n",
    "\n",
    "Example:\n",
    "user: translate en to fr: Where do I find the Twitter Activity Logs API?\n",
    "assistant: Où puis-je trouver l'API des journaux d'activité Twitter?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# llm.create_chat_completion(\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"translate en to fr: How many people from the Netherlands are active on X?\"\n",
    "#         }\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c74778c-1e73-4fa2-a5a3-8c79b55cb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_training_data\n",
    "import langdetect\n",
    "\n",
    "questions, answers, _ = get_training_data()\n",
    "\n",
    "languages = [langdetect.detect(q) for q in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "386d5d77-f050-485d-9e32-446579016854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'API \"Ad Library\" de Meta permet de connaître la cible prévue pour une campagne publicitaire, et permet d'obtenir des informations démographiques sur les personnes ayant été effectivement touchées par cette campagne. La documentation de cette API se trouve à l'adresse suivante : https://www.facebook.com/ads/library/api/. Voici quelques champs pertinents :\n",
      "- 'target_ages', 'target_gender', 'target_locations' : ces champs permettent de comprendre la cible prévue pour la campagne.\n",
      "- 'impressions', 'eu_total_reach', 'estimated_audience_size', 'demographic_distribution', 'age_country_gender_reach_breakdown' : ces champs permettent d'évaluer la performance de la campagne et le profil socio-démographique des personnes ayant interagi avec cette publicité. Certains champs sont soumis à conditions (publicité politique, publicité ayant visé un ou plusieurs pays européens).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(answers[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9755d960-ab80-47a1-9a30-01b369385125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'API \"Ad Library\" de Meta permet de connaître la cible prévue pour une campagne publicitaire, et permet d'obtenir des informations démographiques sur les personnes ayant été effectivement touchées par cette campagne. La documentation de cette API se trouve à l'adresse suivante : https://www.facebook.com/ads/library/api/. Voici quelques champs pertinents :\n",
      "- 'target_ages', 'target_gender', 'target_locations' : ces champs permettent de comprendre la cible prévue pour la campagne.\n",
      "- 'impressions', 'eu_total_reach', 'estimated_audience_size', 'demographic_distribution', 'age_country_gender_reach_breakdown' : ces champs permettent d'évaluer la performance de la campagne et le profil socio-démographique des personnes ayant interagi avec cette publicité. Certains champs sont soumis à conditions (publicité politique, publicité ayant visé un ou plusieurs pays européens).\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Meta Ad Library API allows you to know the intended target for an advertising campaign and obtain demographic information about people who have actually been reached by this campaign. The documentation for this API can be found at the following address: https://www.facebook.com/ads/library/api/. Here are some relevant fields:\n",
      "- 'target_ages', 'target_gender', 'target_locations': these fields allow you to understand the intended target for the campaign. - 'impressions', 'eu_total_reach', 'estimated_audience_size', 'demographic_distribution', 'age_country_gender_reach_breakdown': these fields allow you to evaluate the performance of the campaign and the socio-demographic profile of people who have interacted with this advertising. Some fields are subject to conditions (political advertising, advertising targeting one or more European countries).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3832.14 ms\n",
      "llama_print_timings:      sample time =      55.56 ms /   193 runs   (    0.29 ms per token,  3473.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7786.75 ms /   241 tokens (   32.31 ms per token,    30.95 tokens per second)\n",
      "llama_print_timings:        eval time =   31612.35 ms /   192 runs   (  164.65 ms per token,     6.07 tokens per second)\n",
      "llama_print_timings:       total time =   39889.12 ms /   433 tokens\n"
     ]
    }
   ],
   "source": [
    "translated_qa = []\n",
    "\n",
    "def translate_qa(q, src_language, tgt_language):\n",
    "    answer = llm.create_chat_completion(\n",
    "        messages = [\n",
    "            # {\"role\": \"system\", \"content\": QUESTION_SYSTEM_PROMPT},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Translate from {src_language} to {tgt_language}. Your response should ONLY contain the translation, no other comments or answers. \\n\\n {q}\\n\\n Translation:\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return answer[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "print(answers[1][0])\n",
    "print()\n",
    "print(translate_qa(answers[1][0], \"fr\", \"en\"))\n",
    "\n",
    "# qa_translated = []\n",
    "# for q, a, lang in zip(questions, answers, languages):\n",
    "#     a = a[0]\n",
    "#     if lang == \"en\":\n",
    "#         continue\n",
    "#     print(\"======\")\n",
    "#     # print(q)\n",
    "#     tq = translate_qa(q, lang, \"en\")\n",
    "#     print(tq)\n",
    "#     print(\"-----\")\n",
    "#     # print(a)\n",
    "#     ta = translate_qa(a, lang, \"en\")\n",
    "#     print(ta)\n",
    "    \n",
    "#     qa_translated.append((tq, ta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47557d72-ac47-4f82-95a3-26b351956137",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qa_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19dd0da3-d443-4830-a196-0beb2e3cd6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024, chunk_overlap=0\n",
    ")\n",
    "\n",
    "\n",
    "docs = TextLoader(\"../data/platform-docs-versions/Snapchat_Transparency-World/Transparency Report.md\").load()\n",
    "from IPython.display import display, Markdown\n",
    "# display(Markdown(doc[0].page_content))\n",
    "\n",
    "split = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "751d6ffa-7ee5-453b-a696-ec7e9114d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Resource URL: https://values.snap.com/fr-FR/privacy/transparency\n",
      "Rapport de transparence\n",
      "\n",
      "1er janvier 2023 - 30 juin 2023\n",
      "\n",
      "Publication :\n",
      "\n",
      "25 octobre 2023\n",
      "\n",
      "Mise à jour :\n",
      "\n",
      "25 octobre 2023\n",
      "\n",
      "Pour donner un aperçu des efforts de Snap en matière de sécurité ainsi que de la nature et du volume du contenu signalé sur notre plateforme, nous publions des rapports sur la transparence deux fois par an. Nous entendons poursuivre la publication de ces rapports dans l'objectif de les rendre plus complets et plus documentés, afin que les nombreuses parties prenantes qui s'intéressent de près à nos pratiques en matière de modération de contenu et d'application de la loi, ainsi qu'au bien-être de notre communauté puissent en profiter.\n"
     ]
    }
   ],
   "source": [
    "print(split[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05d68497-dff6-45d7-856e-8e9fcb2ab335",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT=\"\"\"\n",
    "You are an assistant that translates technical markdown documentation from french to english.\n",
    "- French documentation is fed to you in chunks, the only thing you answer is a english translation\n",
    "- Subsequent chunks are all from the same document\n",
    "- If the provided contains just random characters (like an embedded image), you omit this part.\n",
    "\"\"\"\n",
    "\n",
    "def translate(split, chat_log=None):\n",
    "    if not chat_log:\n",
    "        chat_log = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    chat_log.append({\"role\": \"user\", \"content\": split})\n",
    "    answer = llm.create_chat_completion(\n",
    "        messages=chat_log\n",
    "    )\n",
    "    answer = answer[\"choices\"][0][\"message\"]\n",
    "    chat_log.append(answer)\n",
    "    return chat_log\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13b9fb18-e7c1-4454-bd69-c2067e7abfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3832.14 ms\n",
      "llama_print_timings:      sample time =      39.05 ms /   136 runs   (    0.29 ms per token,  3483.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   22159.14 ms /   136 runs   (  162.93 ms per token,     6.14 tokens per second)\n",
      "llama_print_timings:       total time =   22490.46 ms /   137 tokens\n"
     ]
    }
   ],
   "source": [
    "chat_log = None\n",
    "chat_log = translate(split[0].page_content, chat_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c66890fa-6c54-43ff-8ea1-a22c897f7549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3832.14 ms\n",
      "llama_print_timings:      sample time =      44.10 ms /   152 runs   (    0.29 ms per token,  3446.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   24288.03 ms /   152 runs   (  159.79 ms per token,     6.26 tokens per second)\n",
      "llama_print_timings:       total time =   24666.91 ms /   153 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3832.14 ms\n",
      "llama_print_timings:      sample time =      29.07 ms /    99 runs   (    0.29 ms per token,  3405.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4967.88 ms /   152 tokens (   32.68 ms per token,    30.60 tokens per second)\n",
      "llama_print_timings:        eval time =   16911.61 ms /    98 runs   (  172.57 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   22124.62 ms /   250 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3832.14 ms\n",
      "llama_print_timings:      sample time =      45.51 ms /   152 runs   (    0.30 ms per token,  3340.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7604.80 ms /   224 tokens (   33.95 ms per token,    29.46 tokens per second)\n",
      "llama_print_timings:        eval time =   28015.51 ms /   151 runs   (  185.53 ms per token,     5.39 tokens per second)\n",
      "llama_print_timings:       total time =   36012.34 ms /   375 tokens\n"
     ]
    }
   ],
   "source": [
    "chat_log = None\n",
    "translations = []\n",
    "for i in range(3):\n",
    "    chat_log = translate(split[i].page_content, chat_log)\n",
    "    translations.append(chat_log[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e32bb94-dbf8-4308-bb1c-80576da4b323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transparency Report\n",
      "\n",
      "January 1st 2023 - June 30th 2023\n",
      "\n",
      "Publication:\n",
      "October 25th 2023\n",
      "\n",
      "Update:\n",
      "October 25th 2023\n",
      "\n",
      "To give an overview of Snap's security efforts as well as the nature and volume of content reported on our platform, we publish transparency reports twice a year. We aim to continue publishing these reports with more comprehensive and documented information so that the many stakeholders who are closely interested in our content moderation practices and enforcement, as well as the well-being of our community can benefit from them.\n",
      "This transparency report covers the first half of 2021 (January 1st - June 30th). As with our previous reports, we are sharing data on the total number of content in the app, the account level reports we received and acted upon in specific categories of policy violations, how we responded to law enforcement and government requests, and our various sanctions broken down by country. And our enforcement actions broken down by country.\n",
      "As part of our ongoing commitment to improving our transparency reports, we are introducing some new elements with this publication. We have added additional data points around advertising practices and moderation, as well as content and account-related issues. To comply with the EU Digital Services Act, we have also included new contextual information about our operations in EU member states, such as the number of content moderators and monthly active users (MAU) in the region. Much of this information can be found in the report and on our [Transparency Center](https://values.snap.com/fr-FR/privacy/transparency/european-union) page dedicated to the European Union.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(translations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe75f9d-6025-4a87-b409-5e52952d3a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
