# Transparency report

January 1, 2023 - June 30, 2023

Publication :
HH
October 25, 2023

Update :

October 25, 2023

To provide insight into Snap's safety efforts and the nature and volume of content reported on our platform, we publish transparency reports twice a year. We intend to continue publishing these reports with the goal of making them more comprehensive and documented, so that they can benefit the many stakeholders who take a keen interest in our content moderation and law enforcement practices, as well as the well-being of our community.  

This transparency report covers the first half of 2021 (January 1 - June 30). As with our previous reports, we share data on the total number of contents in the app, the account-level reports we received and sanctioned in specific categories of policy violations, how we responded to law enforcement and government requests, and our various sanctions broken down by country. and our enforcement actions broken down by country.  
  
As part of our ongoing commitment to improving our transparency reporting, we are introducing some new elements with this publication. We have added additional data points on our advertising practices and moderation, as well as content and account calls. To comply with the EU's Digital Services Act, we've also added new contextual information about our operations in EU member states, such as the number of content moderators and monthly active users (MAUs) in the region. Much of this information can be found in the report and on our [Transparency Center](https://values.snap.com/fr-FR/privacy/transparency/european-union) page dedicated to the European Union.

Finally, we have updated our [Glossary](https://values.snap.com/fr-FR/privacy/transparency/glossary)with links to our "Explanations of EU rules", which provide additional context on our platform policy and operational efforts.

For more information on our policies to combat harmful content online and our plans to improve our reporting practices, please see our recent [Safety and Impact Blog](https://snap.com/en-US/safety-and-impact?lang=fr-FR) on this transparency report.

To find additional Snapchat privacy and security resources, click on our [About the Transparency Report](https://www.snap.com/en-US/privacy/transparency/about?lang=fr-FR) tab at the bottom of the page.

Please note that the most recent version of this transparency report is available in English (en-US).

### Overview of contents and offending accounts

From January 1 to June 30, 2023, Snap had 6,216,118 pieces of content imposed worldwide that violated our policies.

During said period, we recorded a non-compliant view rate (NCVR) of 0.03%, meaning that out of every 10,000 views of Snap and Story content on Snapchat, three contained content that violated our policies.

\*The correct and consistent application of legislation against false information is a dynamic process that requires up-to-date context and diligence.  As we strive to continually improve the accuracy of our agents' enforcement in this category, since the first half of 2022 we have chosen to report figures in the "Content Enforced" and "Single Accounts Enforced" categories that are estimated based on a rigorous quality assurance review of a statistically significant portion of false information enforcement.  Specifically, we sample a statistically significant portion of the false information enforcement cases in each country and check the quality of the enforcement decisions.  We then use these quality checks to obtain moderation assessments with a 95% confidence interval (margin of error +/- 5%), which we use to calculate the number of moderations for false information reported in the transparency report. 

### Analysis of content and account violations

Our overall reporting and enforcement rates remained fairly similar to those of the previous six months, with a few exceptions in key categories. We observed a decrease of around 3% in the total number of content, reports and moderation over this cycle.

The most important categories were harassment and bullying, spam, weapons and false information. Harassment and bullying increased by around 56% compared to the total number of reports, followed by a nearly 39% increase in content and impositions on the single account. These increases in executions were coupled with an approximate 46% decrease in turnaround time, underscoring our team's operational efficiency for this type of content. Similarly, we saw an increase of around 65% in the total number of spam reports, an increase of almost 110% in content moderations, and an increase of almost 80% in legally enforced unique accounts, and our teams also reduced turnaround time by almost 80%. Our weapons category saw a decrease of around 13% in the total number of reports, a decrease of around 51% in content moderations and a reduction of around 53% in unique accounts imposed. Finally, our fake news category saw an increase of around 14% in the total number of reports, but a decrease of around 78% in content moderations and a decrease of around 74% in unique accounts imposed. This can be attributed to our ongoing quality assurance (QA) process and the use of resources we apply to false information reports to ensure that our moderation teams identify false information and act accurately on the platform.

Overall, while we've seen broadly similar numbers to last period, we believe it's important to continue improving our community's tools to actively and accurately report potential violations as they appear on the platform.

### Combating sexual exploitation and abuse of children

Sexual exploitation of any member of our community, especially minors, is illegal, abhorrent and prohibited under our Community Rules. Preventing, detecting and eradicating child sexual exploitation and abuse (CSEAI) images on our platform is a top priority for Snap, and we are continually developing our capabilities to combat these types of crimes as well as many others.

We use active technology detection tools such as PhotoDNA's robust hashing and Google's child pornography imagery matching, to identify known illegal child sexual abuse images and videos, respectively, and report them to the U.S. Center for Missing and Exploited Children (NCMEC), as required by law. The NCMEC then, in turn, collaborates with national or international law enforcement agencies, as required.

In the first half of 2023, we proactively detected and dealt with 98% of the child sexual exploitation and abuse violations reported here, an increase of 4% on our previous report.

\*\*Please note that each submission to NCMEC may contain multiple pieces of content. The total of individual pieces of media submitted to NCMEC is equal to the total of sanctioned content.

### Terrorist and violent extremist content

During the reporting period (January 1, 2023 to June 30, 2023), we removed 18 accounts for violations of our policy prohibiting terrorist and violent extremist content.

At Snap, we remove terrorist and violent extremist content reported on multiple channels. We encourage users to report terrorist and violent extremist content via our in-app reporting menu, and we work closely with law enforcement to address terrorist and violent extremist content that may appear on Snap.

### Self-harm and suicide content

The mental health and well-being of Snapchatters is important to us, and this has influenced - and continues to influence - our decisions to design Snapchat differently. As a platform designed for communication between true friends, we believe Snapchat can play a unique role in enabling friends to help each other through difficult times.

When our Trust & Safety team recognizes a Snapchatter in distress, they have the ability to pass on resources for self-harm prevention and assistance, and inform emergency response personnel where appropriate. The resources we share are available on our [global safety resource list](https://values.snap.com/fr-FR/safety/safety-resources), and these resources are accessible to all Snapchatters.

### Calls

Starting with this report, we begin reporting the number of calls initiated by users whose accounts have been locked for violating our policies. We only reinstate accounts that our moderators believe have been unjustly locked. During this period, we report calls relating to drug-related content.  In our next report, we will publish more data relating to calls arising from other violations of our policies.

### Ad moderation

Snap is committed to ensuring that all ads comply with our platform policies. We believe in a responsible and respectful approach to advertising, creating a safe and enjoyable experience for all our users. Below, we've included information about our ad moderation. Note that ads on Snapchat can be removed for a variety of reasons as outlined in [Snap's advertising policies](https://snap.com/en-US/ad-policies?lang=fr-FR), including misleading content, adult content, violent or disturbing content, hate speech and intellectual property infringement. In addition, you can now find Snapchat's [Ad Gallery](https://adsgallery.snap.com/?lang=fr-FR) in the navigation bar of this transparency report. 

### Country overview

This section provides an overview of the application of our Community Rules in a sample of geographic regions. Our Rules apply to all Snapchat content - and to all Snapchatters - around the world, regardless of their location.

Country-specific information is available for download via the attached CSV file:

[Download CSV](https://assets.ctfassets.net/kw9k15zxztrs/e2DKlB1h8pa1GVTWezrVT/898af5bce71d35f53064970e55178ae6/Snap_Transparency_Report_H1_2023_Data.csv)