European Union

Last update: October 25, 2023

Welcome to our European Union (EU) transparency page, where we publish EU-specific information required by the EU Digital Services Regulation (DSA).  

Average monthly active beneficiaries 

As of August 1, 2023, we counted 102 million average monthly active recipients ("AMARs") of our [Snapchat] app(http://www.snapchat.com/?lang=fr-FR) in the EU. This means that on average over the last 6 months, 102 million registered users in the EU have opened the Snapchat app at least once in a given month.

These figures break down by member state as follows:

These figures are used to meet current DSA rules and should only be used for DSA purposes. We may change the way we assess this figure over time, including in response to changes in regulators' recommendations and technology. It may also differ from estimates used for other active user figures we publish for other purposes.

Legal representative 

Snap Group Limited has appointed Snap B.V. as its legal representative for the purposes of the DSA. You may contact the representative at dsa-enquiries \[at\] snapchat.com, via our Support Site \[[here](https://help.snapchat.com/hc/requests/new?start=5749439348080640%3Futm_source%3Dweb&utm_medium=snap&utm_campaign=transparency&lang=fr-FR)\], or at :

Snap B.V.  
Keizersgracht 165, 1016 DP  
Amsterdam, The Netherlands

If you are a law enforcement agency, please follow the steps described [here](https://values.snap.com/fr-FR/en-GB/safety/safety-enforcement).

Regulatory authorities

For DSA, we are regulated by the European Commission and the Dutch Authority for Consumers and Markets (ACM). 

DSA transparency report

Snap is required under Articles 15, 24 and 42 of the DSA to publish reports containing prescribed information on Snap's content moderation for Snapchat services that qualify as "online platforms," i.e. Spotlight, For You, Public Profiles, Cards, Lenses and Advertising. This report is to be published every 6 months, starting on October 25, 2023.

Snap publishes transparency reports twice a year to provide information about Snap's security efforts, as well as the nature and volume of content reported on our platform. Our latest report for the first half of 2023 (January 1 - June 30) is available [here](https://values.snap.com/fr-FR/privacy/transparency?lang=en-US). This report includes the following information:

* Government requests, including requests for information and content removal; 
    
* Content violations, including actions taken in relation to illegal content and average response time; 
    
* Appeals, which are received and processed as part of our internal complaints handling process.
    

These sections are relevant to the information required by Article 15.1(a), (b) and (d) of the DSA. Note that they do not yet include a complete data set, as the latest report covers the first half of 2023, which predates the entry into force of the DSA. 

Below we provide some additional information on aspects not covered by our transparency report for the first half of 2023:

Content moderation (Article 15.1(c) and (e), Article 42.2)

All content on Snapchat must adhere to our [Community Rules](https://values.snap.com/fr-FR/privacy/transparency/community-guidelines) and [Terms of Service](https://snap.com/en-US/terms?lang=en-US) and the conditions, rules and explanations therein. Proactive detection mechanisms and reports of illegal or infringing content or accounts prompt a review, in which case our tools systems process the request, collect relevant metadata and route the content to our moderation team via a structured user interface designed to facilitate effective and efficient review operations. When our moderation teams determine, either through human review or automated means, that a user has violated our Terms, we may remove the offending content or account, terminate or limit the visibility of the account in question, and/or notify law enforcement as outlined in our [Snapchat Moderation, Enforcement and Appeals Presentation](https://values.snap.com/fr-FR/privacy/transparency/community-guidelines/moderation).  Users whose accounts are locked by our security team due to violations of the Community Rules may submit a [locked account appeal](https://help.snapchat.com/hc/en-us/articles/17988958753684-How-to-Submit-a-Locked-Account-Appeal?lang=fr-FR#:~:text=Locked%20account%20appeals%20can%20be,to%20start%20the%20appeal%20process.), and users may [appeal certain content enforcement actions](https://help.snapchat.com/hc/en-us/articles/18120518120340-How-to-Submit-a-Content-Appeal?lang=fr-FR).

Automated content moderation tools

On our public content surfaces, content typically goes through both automated moderation and human review before it can be distributed to a wide audience. Automated tools include:

* Proactive detection of illegal and infringing content using machine learning ;
    
* Hash tools (such as PhotoDNA and Google's CSAI Match);
    
* Abusive language detection to refuse content based on an identified and regularly updated list of abusive keywords, including emojis.
    

During the period of our last [Transparency Report](https://values.snap.com/fr-FR/privacy/transparency?lang=en-US) (H1 2023), it was not necessary to gather formal indicators/error rates for these automated systems. However, we regularly monitor these systems for problems, and our human moderation measures are regularly designed for accuracy.

  
Human moderation

Our content moderation team operates worldwide, enabling us to guarantee the safety of Snapchatters 24/7. Below is a breakdown of our human moderation resources by moderator language specialties (please note that some moderators specialize in multiple languages) as of August 2023 :

The above figures fluctuate, as we display trends in incoming volume or submissions by language/country. In situations where we need additional language assistance, we use translation services.

Moderators are recruited on the basis of a standard job description which includes a language requirement (depending on the need). The language requirement stipulates that the candidate must be able to demonstrate written and oral fluency and have at least one year's professional experience for entry-level positions. Candidates must also meet the training and experience requirements to be considered. Candidates must also demonstrate an understanding of current events for the country or region in which they are involved in content moderation. 

Our moderation team applies our policies and enforcement measures to help protect our Snapchat community. Training takes place over several weeks, and new team members are trained in Snap's policies, tools and escalation procedures. After training, each moderator must pass a certification exam before being allowed to review content. Our moderation team regularly participates in refresher training relevant to their workflows, particularly when we're dealing with borderline cases and context-dependent situations. We also run refresher programs, certification sessions and quizzes to ensure that all moderators are up to date and compliant with all updated policies. Finally, when urgent content trends surface on ongoing events, we quickly issue policy clarifications so that teams can respond according to Snap's policies.

We provide our content moderation team, Snap's "digital first responders," with significant support and resources, including workplace wellness support and easy access to mental health services. 

  
Content moderation safeguards

We recognize that there are risks associated with content moderation, including risks to freedom of expression that may be caused by the bias of automated and human moderators and abusive reporting, including by governments, political constitutions or well-organized individuals. Snapchat is generally not a place for political or activist content, especially in our public spaces. 

Nevertheless, to prevent these risks, Snap has implemented testing and training, and has robust and consistent procedures for handling reports of illegal content or content in violation of the law, including to government authorities. We are constantly evaluating and improving our content moderation algorithms. Although it is difficult to detect potential infringements of freedom of expression, we are not aware of any significant problems, and we provide our users with ways of reporting any errors.

Our policies and systems promote consistent and fair application and, as described above, offer Snapchatters the opportunity to meaningfully challenge application results through notification and appeal procedures that aim to safeguard the interests of our community while protecting Snapchatters' individual rights.

We are constantly striving to improve our enforcement policies and procedures. We've also made great strides in combating potentially harmful and illegal content and activity on Snapchat. This is reflected in the increase in our reporting and enforcement numbers from our latest [Transparency Report](https://values.snap.com/fr-FR/privacy/transparency) and the decrease in prevalence rates for violations on Snapchat as a whole. 

  
Notifications from trusted reporters (Article 15.1(b))

During the period of our last [Transparency Report](https://values.snap.com/fr-FR/en-GB/privacy/transparency) (H1 2023), no trusted reporters were officially designated under the DSA. As a result, the number of notifications submitted by these trusted signalers was zero (0) during this period.

  
Out-of-court disputes (Article 24.1(a))

During the period of our last [Transparency Report](https://values.snap.com/fr-FR/en-GB/privacy/transparency) (H1 2023), no out-of-court dispute settlement bodies were officially designated under the DSA As a result, the number of disputes submitted to these bodies was zero (0) during this period.

  
Account suspensions under Article 23 (Article 24.1(b))

During the period of our last [Transparency Report](https://values.snap.com/fr-FR/en-GB/privacy/transparency) (H1 2023), it was not necessary to suspend accounts under Article 23 of the DSA for the provision of manifestly illegal content, unfounded opinions or unfounded complaints. As a result, the number of such suspensions was zero (0). However, Snap takes appropriate enforcement action against accounts as detailed in our [Explanatory Note on Snapchat Moderation, Enforcement and Appeals](https://values.snap.com/fr-FR/en-GB/privacy/transparency/community-guidelines/moderation) and information on the level of Snap account enforcement can be found in our [Transparency Report](https://values.snap.com/fr-FR/en-GB/privacy/transparency) (H1 2023).