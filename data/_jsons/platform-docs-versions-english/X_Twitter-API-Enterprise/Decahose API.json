{
    "0": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\nContent: \nDecahose stream  \nThis endpoint has been updated to include Tweet edit metadata. Learn more about these metadata on the [\"Edit Tweets\" fundamentals page](https://developer.twitter.com/en/docs/twitter-api/enterprise/edit-tweets).\n",
        "line_start": 0,
        "line_end": 2
    },
    "1": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tDecahose stream\n\nContent: \n## Decahose stream  \n[Enterprise](https://developer.twitter.com/en/products/twitter-api/enterprise)  \n_This is an enterprise API available within our managed access levels only. To use this API, you must first set up an account with our enterprise sales team.\u00a0[Learn more](https://developer.twitter.com/en/products/twitter-api/enterprise)_  \nThe Decahose delivers a 10% random sample of the realtime Twitter Firehose through a streaming connection. This is accomplished via a realtime sampling algorithm which randomly selects the data, while still allowing for the expected low-latency delivery of data as it is sent through the firehose by Twitter.  \nBelow are some of the features available with Decahose:  \n* **Expanded and enhanced URLs:**\u00a0- fully unwinds shortened URLs and provides additional metadata (page title and description)\n* **Stream partitioning**\u00a0- 2 partitions, each containing 50% of volume of the Decahose stream\n* **Enhanced reliability**\u00a0- geographic diversity of backend systems  \nNote: This data is delivered in bulk, and does not support additional filtering (e.g. for keywords).  \nStreaming likes  \nEnterprise\n",
        "line_start": 4,
        "line_end": 15
    },
    "2": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tStreaming likes\n\nContent: \n# Streaming likes  \n_This is an enterprise API available within our managed access levels only. To use this API, you must first set up an account with our enterprise sales team.\u00a0[Learn more](https://developer.twitter.com/en/products/twitter-api/enterprise)_  \nLikes enable insight into who likes Tweets and delivers accurate counts of likes. Gnip\u2019s Firehose and Decahose can deliver public likes related to the Tweets delivered via Gnip. This yields real-time public engagement and audience metrics associated with a Tweet.\n",
        "line_start": 24,
        "line_end": 27
    },
    "3": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tStreaming likes\n\tGetting started with Likes\n\nContent: \n## Getting started with Likes  \nAs you prepare to consume likes data, you should know that:  \n* Likes are delivered via an independent, separate stream\n* Likes are historically referred to as \u201cFavorites\u201d. The enriched native format payload maintains this nomenclature\n* Streams include only public likes\n* Public means that the liking user, Tweet creator and Tweet are all public on the platform\n* Likes are very similar to Retweets and represent a public signal of engagement\n* Payload elements include:\n* Original Tweet object\n* Actor object that created the original Tweet\n* Actor object that performed the like action\n* Only original content can be liked\n* Retweets cannot be liked. A like of a Retweet is applied to the original Tweet\n* Quoted Tweets\u00a0_can_\u00a0be liked\n* Like activities include applicable Gnip Enrichments (where purchased/applied)\n* Supported Products / Features\n* Likes streams support Backfill (where purchased/applied)\n* There is no Replay support for likes streams\n* There is no Search or Historical support for likes\n* There are no immediate plans to add likes support to PowerTrack\n",
        "line_start": 31,
        "line_end": 51
    },
    "4": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tStreaming likes\n\tDecahose\n\nContent: \n## Decahose  \n* For the 10% sample Tweets delivered in the Decahose, stream includes 100% of the applicable public likes\n* **Partitions:**\u00a02\n* **URL Structure**\n* https://gnip-stream.twitter.com/stream/sample10-likes/accounts/<accountName>/publishers/twitter/<streamLabel>.json?partition=1\n",
        "line_start": 54,
        "line_end": 59
    },
    "5": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tStreaming likes\n\tNative enriched format payload\n\nContent: \n## Native enriched format payload  \n`{      \"id\":\"43560406e0ad9f68374445f5f30c33fc\",    \"created_at\":\"Thu Dec 01 22:27:39 +0000 2016\",    \"timestamp_ms\":1480631259353,    \"favorited_status\":{         \"created_at\":\"Thu Dec 01 22:27:16 +0000 2016\",       \"id\":804451830033948672,       \"id_str\":\"804451830033948672\",       \"text\":\"@kafammheppduman\",       \"source\":\"\\u003ca href=\\\"http:\\/\\/twitter.com\\/download\\/android\\\" rel=\\\"nofollow\\\"\\u003eTwitter for Android\\u003c\\/a\\u003e\",       \"truncated\":false,       \"in_reply_to_status_id\":803694205163814912,       \"in_reply_to_status_id_str\":\"803694205163814912\",       \"in_reply_to_user_id\":2855759795,       \"in_reply_to_user_id_str\":\"2855759795\",       \"in_reply_to_screen_name\":\"kafammheppduman\",       \"user\":{            \"id\":2855759795,          \"id_str\":\"2855759795\",          \"name\":\"delirdim kanka\",          \"screen_name\":\"kafammheppduman\",          \"location\":\"sanane\",          \"url\":\"http:\\/\\/instagram.com\\/kafammheppduman\",          \"description\":\"Manit @GalatasaraySk \\ud83d\\udc9e\",          \"translator_type\":\"none\",          \"protected\":false,          \"verified\":false,          \"followers_count\":3702,          \"friends_count\":607,          \"listed_count\":1,          \"favourites_count\":113338,          \"statuses_count\":389,          \"created_at\":\"Sat Nov 01 22:38:25 +0000 2014\",          \"utc_offset\":null,          \"time_zone\":null,          \"geo_enabled\":true,          \"lang\":\"tr\",          \"contributors_enabled\":false,          \"is_translator\":false,          \"profile_background_color\":\"C0DEED\",          \"profile_background_image_url\":\"\",          \"profile_background_image_url_https\":\"\",          \"profile_background_tile\":false,          \"profile_link_color\":\"1DA1F2\",          \"profile_sidebar_border_color\":\"C0DEED\",          \"profile_sidebar_fill_color\":\"DDEEF6\",          \"profile_text_color\":\"333333\",          \"profile_use_background_image\":true,        \"Profile_image_url\": \"http:\\/\\/pbs.twimg.com\\/profile_images\\/804421763945861121\\/v3bp9pnq_normal.jpg\",          \"Profile_image_url_https\": \"https:\\/\\/pbs.twimg.com\\/profile_images\\/804421763945861121\\/v3bp9pnq_normal.jpg\",          \"profile_banner_url\":\"https:\\/\\/pbs.twimg.com\\/profile_banners\\/2855759795\\/1480630085\",          \"default_profile\":true,          \"default_profile_image\":false,          \"following\":null,          \"follow_request_sent\":null,          \"notifications\":null       },       \"geo\":null,       \"coordinates\":null,       \"place\":null,       \"contributors\":null,       \"is_quote_status\":false,       \"retweet_count\":0,       \"favorite_count\":0,       \"entities\":{            \"hashtags\":[],          \"urls\":[],          \"user_mentions\":[               {                  \"screen_name\":\"kafammheppduman\",                \"name\":\"delirdim kanka\",                \"id\":2855759795,                \"id_str\":\"2855759795\",                \"indices\":[                     0,                   16                ]             }          ],          \"symbols\":[]       },       \"favorited\":false,       \"retweeted\":false,       \"filter_level\":\"low\",       \"lang\":\"und\"    },    \"user\":{         \"id\":774146932365070336,       \"id_str\":\"774146932365070336\",       \"name\":\"Uyuyan Adam\",       \"screen_name\":\"saykoMenn\",       \"location\":\"Tarsus, T\\u00fcrkiye\",       \"url\":\"http:\\/\\/connected2.me\\/pmc1i\",       \"description\":null,       \"translator_type\":\"none\",       \"protected\":false,       \"verified\":false,       \"followers_count\":414,       \"friends_count\":393,       \"listed_count\":0,       \"favourites_count\":9868,       \"statuses_count\":370,       \"created_at\":\"Fri Sep 09 07:26:26 +0000 2016\",       \"utc_offset\":null,       \"time_zone\":null,       \"geo_enabled\":false,       \"lang\":\"tr\",       \"contributors_enabled\":false,       \"is_translator\":false,       \"profile_background_color\":\"F5F8FA\",       \"profile_background_image_url\":\"\",       \"profile_background_image_url_https\":\"\",       \"profile_background_tile\":false,       \"profile_link_color\":\"1DA1F2\",       \"profile_sidebar_border_color\":\"C0DEED\",       \"profile_sidebar_fill_color\":\"DDEEF6\",       \"profile_text_color\":\"333333\",       \"profile_use_background_image\":true,       \"Profile_image_url\": \"http:\\/\\/pbs.twimg.com\\/profile_images\\/802992813424201728\\/VMzcTL3x_normal.jpg\",       \"Profile_image_url_https\": \"https:\\/\\/pbs.twimg.com\\/profile_images\\/802992813424201728\\/VMzcTL3x_normal.jpg\",       \"profile_banner_url\":\"https:\\/\\/pbs.twimg.com\\/profile_banners\\/774146932365070336\\/1480283382\",       \"default_profile\":true,       \"default_profile_image\":false,       \"following\":null,       \"follow_request_sent\":null,       \"notifications\":null    } }`\n",
        "line_start": 62,
        "line_end": 64
    },
    "6": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tStreaming likes\n\tLike Delete / \u201cUnlike\u201d payload\n\nContent: \n## Like Delete / \u201cUnlike\u201d payload  \n`{      \"delete\":{         \"favorite\":{            \"tweet_id\":696615514970279937,          \"tweet_id_str\":\"696615514970279937\",          \"user_id\":2510287578,          \"user_id_str\":\"2510287578\"       },       \"timestamp_ms\":\"1480437031205\"    } }`\n",
        "line_start": 67,
        "line_end": 69
    },
    "7": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tStreaming likes\n\tCompliance guidance\n\nContent: \n## Compliance guidance  \n* Compliance only possible via Gnip Compliance Firehose\n* If a tweet is deleted, all likes associated with that Tweet should be deleted (just like its Retweets)\n* We will not deliver individual delete events for each Like in this scenario\n* If a user protects/deletes his/her account, all likes performed by that user should be scrubbed (just like their Retweets)\n* We will not deliver individual delete events for each like in this scenario\n* If a user protects/deletes his/her account, all likes of that user\u2019s Tweets should be scrubbed (just like their Tweets and Retweets)\n* We will not deliver individual delete events for each like in this scenario\n* If a user scrubs their geo, all likes associated with their Tweets should have its geo scrubbed (because the payload contains their original Tweet)\n* If a user removes a like they performed (aka unliked), this is equivalent to a delete of that like and that like should be deleted.\n* We will deliver an event in the Compliance Firehose to reflect these like deletions\n* These like delete events will not include an ID for the original like event and will instead include the TweetID that was liked and the User that performed that like (a user can only like a Tweet once at a time)  \nRecovery and redundancy\n",
        "line_start": 72,
        "line_end": 85
    },
    "8": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tStreaming likes\n\tIntroduction\n\nContent: \n## Introduction  \nWith streaming high volumes of realtime Tweets comes a set of best practices that promote both data reliability and data full-fidelity. When consuming realtime data, maximizing your connection time is a fundamental goal. When disconnects occur, it is important to automatically detect that and reconnect. After reconnecting it\u2019s important to assess if there are any periods to backfill data for. The component that manages these details and consumes realtime Tweets is only one part of a system with network, datastore, server, and storage concerns. Given the complexity of these systems, another best practice is to have different streaming environments, with at least separate streams for development/testing and production.  \nDecahose comes with a set of features that help with these efforts.  \n1. To support multiple environments, we can deploy\u00a0[Additional Streams](#AdditionalStreams)\u00a0for your account. These streams are independent of each other and have a different stream\\_label\u00a0 to help differentiate them.\n2. To help support maintaining a connection, each Decahose stream supports\u00a0[Redundant Connections](https://developer.twitter.com/en/docs/twitter-api/enterprise/decahose-api/guides/recovery-and-redundancy.html#RedundantConnections). The most common architecture is for a stream to have two connections, and on the client-side there are two independent consumers \u2013ideally on different networks. With this design, there can be redundancy across the client-side networks, servers, and datastore pathways. Note that a full-copy of the data is served on each connection and the client-side must be tolerant of and manage duplicate data.\n3. A '**heartbeat**' will be provided every 10 seconds; however, with the Decahose stream, the volume of data is high enough that even a small duration (e.g., a few seconds) of no Tweets can indicate a connection issue. Therfore, both a 'data silence' and lack of a heartbeat can be used to detect a disconnect.  \nSince disconnects will happen, the Decahose stream has a dedicated\u00a0[Recovery](https://developer.twitter.com/en/docs/tweets/sample-realtime/guides/recovery-and-redundancy#Replay)\u00a0and a\u00a0[Backfill](https://developer.twitter.com/en/docs/tweets/sample-realtime/guides/recovery-and-redundancy#Backfill)\u00a0feature to help recover data that was missed due to disconnections and other operational issues.\n",
        "line_start": null,
        "line_end": null
    },
    "9": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tStreaming likes\n\tAdditional Streams\n\nContent: \n## Additional Streams  \nHaving additional Decahose streams is another way to help build reliability into your solution. Any additional streams are completely independent, having their unique endpoint. Each stream is assigned its own stream\\_label, and this label, along with your account name, are part of that stream\u2019s URL. See the example below:  \nhttps://gnip-stream.twitter.com/stream/sample10/accounts/:account\\_name/publishers/twitter/:stream\\_label.json  \nThe most common convention is to have a realtime stream dedicated for you production system, and an additional stream available for development and testing. Having a test/development stream enables Decahose customers to have a stream to test client consumer updates. While any (unique) label can be assigned to a stream, one convention is to use \u2018prod\u2019 for production stream, and \u2018dev\u2019 or \u2018sandbox\u2019 for an additional development stream.  \nThe number of streams, and their unique labels, is configurable by your account representative.\n",
        "line_start": 100,
        "line_end": 105
    },
    "10": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tStreaming likes\n\tRedundant Connections\n\nContent: \n## Redundant Connections  \nA redundant connection simply allows you to establish more than one simultaneous connection to the data stream. This provides redundancy by allowing you to connect to the same stream with two separate consumers, receiving the same data through both connections. Thus, your app has a hot failover for various situations, e.g. where one stream is disconnected or where your app\u2019s primary server fails.  \nThe number of connections allowed for any given stream is configurable by your account representative. To use a redundant stream, simply connect to the same URL used for your primary connection. The data for your stream will be sent through both connections, with both stream connections represented on the stream dashboard.  \nNote that for billing purposes, we deduplicate the activity counts you receive through multiple connections such that you are only billed for each unique activity once. Given the Decahose has two partitions, here's an example of how the connection count works below:  \nConnect to decahose partition=1\nConnect to decahose partition=1\nConnect to decahose partition=2  \nThe above situation yields a total of three connections \u2013 two connections to partition=1 and one connection to partition=2. Normally, you would want the same number of connections to each partition, so this example highlights a situation where the redundant connection to partition=2 has dropped and you want to further invstigate.\n",
        "line_start": null,
        "line_end": null
    },
    "11": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tStreaming likes\n\tRecovery\n\nContent: \n## Recovery  \n#### Overview  \nRecovery is a data recovery tool (not to be used for primary data collection) that provides streaming access to a rolling 5-day window of recent Twitter historical data. It should be utilized to recover data in scenarios where your consuming application misses data in the realtime stream, whether due to disconnecting for a short period, or for any other scenario where you fail to ingest realtime data for a period of time.  \n#### Using Recovery  \nWith the Recovery stream, your app can make requests to it that operate in the same manner as requests to the realtime streams. However, your app must specify parameters in the URL that indicate the time window you are requesting. In other words, a Recovery request asks the API for \"Tweets from time A to time B.\" These Tweets are then delivered through your streaming connection in a manner that mimics the realtime stream, but at a slightly slower-than-realtime rate. See below for example:  \n\"https://stream-data-api.twitter.com/stream/powertrack/accounts/someAccountName/publishers/twitter/powertrack.json?startTime=2023-07-05T17:09:12.070Z\"  \nTweets are delivered beginning with the first (oldest) minute of the specified time period, continuing chronologically until the final minute is delivered. At that point, aRecovery Request Completed\u00a0message is sent through the connection, and the connection is then closed by the server. If your request begins at a time of day where little or no matching results occurred, there will likely be some period of time before the first results are delivered \u2013 data will be delivered when Recovery encounters matches in the portion of the archive being processed at that time. When no results are available to deliver, the stream will continue sending carriage-return, or \"heartbeats\", through the connection to prevent you from timing out.  \nRecovery is intended as a tool for easily recovering data missed due to short disconnects, not for very long time periods like an entire day. If the need to recover data for long periods arises, we recommend breaking longer requests into shorter time windows (e.g. two hours) to reduce the possibility of being disconnected mid-request due to internet volatility or other reasons, and to provide more visibility into the progress of long requests.  \n#### Data Availability  \nYou can use the Recovery feature to recover missed data within the last 24 hours if you are unable to reconnect with the 5 minute backfill window.  \nThe streaming recovery feature allows you to have an extended backfill window of 24 hours. Recovery enables you to \u2018recover\u2019 the time period of missed data. A recovery stream is started when you make a connection request using 'start\\_time' and 'end\\_time' request parameters. Once connected, Recovery will re-stream the time period indicated, then disconnect.  \nYou will be able to make 2 concurrent requests to recovery at the same time, i.e. \u201ctwo recovery jobs\u201d. Recovery works technically in the same way as backfill, except a start and end time is defined. A recovery period is for a single time range.\n",
        "line_start": 124,
        "line_end": 136
    },
    "12": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tStreaming likes\n\tBackfill\n\nContent: \n## Backfill  \nTo request backfill, you need to add a backfillMinutes=N parameter to your connection request, where N is the number of minutes (1-5, whole numbers only) to backfill when the connection is made. For example, if you disconnect for 90 seconds, you should add backfillMinutes=2 to your connection request. Since this request will provide backfill for 2 minutes, including for the 30-second period before you disconnected, your\u00a0_consumer app must be tolerant of duplicate data_.  \nAn example Decahose connection request URL, requesting a 5 minute backfill to partition 1, looks like:  \nhttps://gnip-stream.twitter.com/stream/sample10/accounts/:account\\_name/publishers/twitter/:stream\\_label.json?partition=1&backfillMinutes=5  \n**NOTES:**  \n* You do have the option to always use \u2018backfillMinutes=5\u2019 when you connect, then handling any duplicate data that is provided.  \n* If you are disconnected for more than five minutes, you can recover data using Recovery.\n",
        "line_start": 149,
        "line_end": 156
    },
    "13": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tStreaming likes\n\tRecovering from Disconnect\n\nContent: \n## Recovering from Disconnect  \nRestarting and recovering from a disconnect involves several steps:  \n* Determining length of disconnect time period.\n* 5 minutes or less?\n* If you have Backfill enabled for stream, prepare connection request with appropriate \u2018backfillMinutes\u2019 parameter.\n* More than 5 minutes?\n* If you have a Recovery stream, make a Recovery request for the disconnected time period (ideally with your current realtime rule set, using the Rules API if necessary).\n* Request a new connection.  \nWhen you experience disconnects or downtime, here are strategies\u00a0to mitigate and recover in this scenario:  \n1. **Implement backfill**\nBackfill lets you reconnect from a point prior to disconnecting from a stream connection, and covers disconnects of up to 5 minutes. It is implemented by including a parameter in the connection request.  \n2. **Consume a redundant stream from another location**\nIf the redundant stream can be streamed into the same live environment, deduplicating data, you will eliminate the need for recovery unless BOTH the normal stream and redundant stream experience simultaneous downtime or disconnects. If the redundant stream cannot be streamed live into the production environment, it can be written into a separate \u201cemergency\u201d data store. Then, in the event of disconnects or downtime on the primary stream connection, your system will have data on hand to fill in your primary database for the period of time where data is missing  \n3. **Implement Recovery**\nWhere disconnects or downtime affect both the primary stream and redundant stream, use the Decahose Recovery to recover any data missed. The API provides a rolling window covering 5 days of the archive, and would be best utilized by requesting no more than an hour of that window at a time, and streaming it in. This is done in parallel to the realtime stream. Note that we do not have solutions for recovering Decahose data from beyond the 5 day window provided by Recovery, so it is important for you to utilize a redundant stream to ensure you have a complete copy of data on your side in the case of significant downtime on your side.  \nWhen you detect abnormal stored data volumes-\nPotential ways you can detect missing data where no disconnects or downtime occurred\u2026  \n1. Count inbound Tweets\nYour system should count the raw number of Tweets you receive at the very beginning of your ingestion app, and then provide a way to compare those numbers to the number of Tweets that reaches your final data store. Any differences can be monitored, and alert your team to issues causing data to be dropped after it is received.  \n2. Analyze for abnormal stored volumes\nYou may also want to analyze the volumes of stored data in your final database to look for abnormal drops. This can indicate issues as well, although there will likely be circumstances in which drops in volume are normal (e.g. if the Twitter platform is unavailable and people are not able to create Tweets for some period of time).  \nDecahose stream  \ndecahose\n",
        "line_start": null,
        "line_end": null
    },
    "14": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tDecahose stream\n\nContent: \n# Decahose stream  \nJump to on this page  \n[Methods](#Methods)  \n[Authentication](#Authentication)  \n[GET /{stream-type}/:stream](#Stream)  \n[Replay API](#replay_api)\n",
        "line_start": 201,
        "line_end": 207
    },
    "15": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tDecahose stream\n\tMethods\u00a0[\u00b6](#methods- \"Permalink to this headline\")\n\nContent: \n## Methods\u00a0[\u00b6](#methods- \"Permalink to this headline\")  \n| Method | Description |\n| --- | --- |\n| [GET /{stream-type}/:stream](#Stream) | Connect to the data stream |\n",
        "line_start": 213,
        "line_end": 217
    },
    "16": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tDecahose stream\n\tAuthentication\u00a0[\u00b6](#authentication- \"Permalink to this headline\")\n\nContent: \n## Authentication\u00a0[\u00b6](#authentication- \"Permalink to this headline\")  \nAll requests to the Volume Stream APIs must use HTTP Basic Authentication, constructed from a valid email address and password combination used to log into your account at console.gnip.com. Credentials must be passed as the Authorization header for each request. So confirm that your client is adding the \"Authentication: Basic\" HTTP header (with encoded credentials over HTTPS) to all API requests.\n",
        "line_start": 219,
        "line_end": 221
    },
    "17": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tDecahose stream\n\tGET /{stream-type}/:stream\u00a0[\u00b6](#get-stream-type-stream- \"Permalink to this headline\")\n\nContent: \n## GET /{stream-type}/:stream\u00a0[\u00b6](#get-stream-type-stream- \"Permalink to this headline\")  \nEstablishes a persistent connection to the Firehose stream, through which the realtime data will be delivered.  \n**Note:** Please see [this article](https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data) for additional details on consuming streaming data after the connection is established.  \n#### Request Specifications  \n|     |     |\n| --- | --- |\n| **Request Method** | HTTP GET |\n| **Connection Type** | Keep-Alive  <br>  <br>This should be specified in the header of the request. |\n| **URL** | Found on the stream's API Help page of your dashboard, using the following structure:  <br>  <br>Decahose:<br><br>[https://gnip-stream.twitter.com/stream/sample10/accounts/:account\\_name/publishers/twitter/:stream\\_label.json?partition=1](https://gnip-stream.twitter.com/stream/sample10/accounts/:account_name/publishers/twitter/:stream_label.json?partition=1) |\n| **Partition (required)** | `partition={#}` - Partitioning is now required in order to consume the full stream. You will need to connect to the stream with the partition parameter specified. Below is the number of partitions per stream:<br><br>* Decahose: 2 partitions |\n| **Compression** | Gzip. To connect to the stream using Gzip compression, simply send an Accept-Encoding header in the connection request. The header should look like the following:  <br>  <br>Accept-Encoding: gzip |\n| **Character Encoding** | UTF-8 |\n| **Response Format** | JSON. The header of your request should specify JSON format for the response. |\n| **Rate Limit** | 10 requests per 60 seconds. |\n| **Backfill Parameter** | If you have purchased a stream with Backfill enabled, you'll need to add the \"backfillMinutes\" parameter into GET request to enable it. |\n| **Read Timeout** | Set a read timeout on your client, and ensure that it is set to a value beyond 30 seconds. |\n| **Support for Tweet edits** | All Tweet objects will include Tweet edit metadata describing the Tweet's edit history. See the [\"Edit Tweets\" fundamentals page](https://developer.twitter.com/en/docs/twitter-api/enterprise/edit-tweets) for more details. |  \n#### Responses  \nThe following responses may be returned by the API for these requests. Most error codes are returned with a string with additional details in the body. For non-200 responses, clients should attempt to reconnect.  \n| Status | Text | Description |\n| --- | --- | --- |\n| 200 | Success | The connection was successfully opened, and new activities will be sent through as they arrive. |\n| 401 | Unauthorized | HTTP authentication failed due to invalid credentials. Log in to console.gnip.com with your credentials to ensure you are using them correctly with your request. |\n| 406 | Not Acceptable | Generally, this occurs where your client fails to properly include the headers to accept gzip encoding from the stream, but can occur in other circumstances as well.  <br>  <br>Will contain a JSON message similar to \"This connection requires compression. To enable compression, send an 'Accept-Encoding: gzip' header in your request and be ready to uncompress the stream as it is read on the client end.\" |\n| 429 | Rate Limited | Your app has exceeded the limit on connection requests. |\n| 503 | Service Unavailable | Twitter server issue. Reconnect using an exponential backoff pattern. If no notice about this issue has been posted on the [Twitter API Status Page](https://api.twitterstat.us/), contact support or emergency if unable to connect after 10 minutes. |  \n#### Example curl Request  \nThe following example request is accomplished using cURL on the command line. However, note that these requests may also be sent with the programming language of your choice:  \ncurl --compressed -v -uexample@customer.com \"https://gnip-stream.twitter.com/stream/firehose/accounts/:account\\_name/publishers/twitter/:stream\\_label.json?partition={#}\"\n",
        "line_start": 223,
        "line_end": 252
    },
    "18": {
        "content": "Source document: data/platform-docs-versions-english/X_Twitter-API-Enterprise/Decahose API.md\nParagraph location: \n\tDecahose stream\n\tReplay API \u00a0[\u00b6](#replay-api- \"Permalink to this headline\")\n\nContent: \n## Replay API \u00a0[\u00b6](#replay-api- \"Permalink to this headline\")  \nThe Replay API is an important complement to realtime Volume streams. Replay is a data recovery tool that provides streaming access to a rolling window of recent Twitter historical data.\n",
        "line_start": 263,
        "line_end": 265
    }
}